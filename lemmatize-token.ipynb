{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mstudio/miniconda3/envs/py3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import os\n",
    "from ast import literal_eval\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english')\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/mstudio/Library/CloudStorage/Box-Box/coolie/token/'\n",
    "df=pd.read_csv(path+'/coolie-token.csv', converters={'state':literal_eval, 'context':literal_eval, 'city':literal_eval, 'sent':literal_eval, 'date':literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17950630 19631206\n"
     ]
    }
   ],
   "source": [
    "print(df['date'].min(), df['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['state', 'city', 'date', 'lccn', 'sent', 'title'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n):\n",
    "    entire=[]\n",
    "    n_grams=ngrams(word_tokenize(text), n)\n",
    "    for i in n_grams:\n",
    "        entire.append(i)\n",
    "    return entire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['context_str']=df['context'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(dataframe:pd.DataFrame()):\n",
    "    dataframe['alpha']=dataframe['sent'].apply(lambda token_list: [token for token in token_list if token.isalpha()])\n",
    "    dataframe['stop']=dataframe['alpha'].apply(lambda token_list: [token for token in token_list if token.lower() not in stop])\n",
    "    dataframe['lemma']=dataframe['stop'].apply(lambda token_list: [token.lemma_ for token in nlp(\" \".join(token_list))])\n",
    "    dataframe.drop(['alpha', 'stop'], axis=1, inplace=True)\n",
    "    return dataframe\n",
    "    # dataframe['punct_word']=dataframe['sent'].apply(lambda row: ' '.join([w.lemma_ for w in nlp(row)]))\n",
    "    # dataframe['sent_stop']=dataframe['punct_word'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    # dataframe['sent_punct']=dataframe['sent_stop'].str.replace('[^\\w\\s]','')\n",
    "    # dataframe['sent_lemma']=dataframe['sent_punct'].apply(lambda x: ' '.join(x for x in x.split() if x.isalpha()))\n",
    "    # dataframe['sent_token']=dataframe['sent_lemma'].apply(word_tokenize)\n",
    "    # dataframe.drop(['sent_stop', 'sent_punct', 'punct_word'], axis=1, inplace=True)\n",
    "    # return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reuse_df(data:pd.DataFrame()):\n",
    "    no_reuse=[]\n",
    "    pair_index1=[]\n",
    "    pair_index2=[]\n",
    "    pair_date1=[]\n",
    "    pair_date2=[]\n",
    "    pair_text1=[]\n",
    "    pair_text2=[]\n",
    "    pair_state1=[]\n",
    "    pair_state2=[]\n",
    "    for k in tqdm(range(0,data.shape[0])):\n",
    "        t=0\n",
    "        for p in range(t,data.shape[0]):\n",
    "            t+=1\n",
    "            if t>k:\n",
    "                if (k==p) & (len(list(set(data['5-gram'][k]).intersection(data['5-gram'][p]))) == len(data['5-gram'][k])):\n",
    "                    pass\n",
    "                elif len(list(set(data['5-gram'][k]).intersection(data['5-gram'][p]))) == 0:\n",
    "                    pass\n",
    "                elif len(list(set(data['5-gram'][k]).intersection(data['5-gram'][p]))) >= 3:\n",
    "                    no_reuse.append(len(list(set(data['5-gram'][k]).intersection(data['5-gram'][p]))))\n",
    "                    pair_index1.append(k) \n",
    "                    pair_index2.append(p)\n",
    "                    pair_date1.append(data.iloc[k]['date'])\n",
    "                    pair_date2.append(data.iloc[p]['date'])\n",
    "                    pair_text1.append(data.iloc[k]['lemma'])\n",
    "                    pair_text2.append(data.iloc[p]['lemma'])\n",
    "                    pair_state1.append(data.iloc[k]['state'])\n",
    "                    pair_state2.append(data.iloc[p]['state'])\n",
    "    return pd.DataFrame({'no_reuse':no_reuse, 'pair_index1':pair_index1, 'pair_index2':pair_index2, \n",
    "             'pair_date1':pair_date1, 'pair_date2':pair_date2, \n",
    "              'pair_text1':pair_text1, 'pair_text2':pair_text2,\n",
    "                'pair_state1':pair_state1, 'pair_state2':pair_state2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=lemmatization(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['state', 'city', 'date', 'lccn', 'sent', 'title', 'lemma'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path+'coolie-token-lemma.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/mstudio/Library/CloudStorage/Box-Box/coolie/sent/'\n",
    "df=pd.read_csv(path+'/coolie-lemma.csv', converters={'state':literal_eval, 'context':literal_eval, 'city':literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125253, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma_str']=df['lemma'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['5-gram']=df['lemma_str'].apply(lambda x: get_ngrams(x, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125253/125253 [23:48:09<00:00,  1.46it/s]  \n"
     ]
    }
   ],
   "source": [
    "reprintdf=reuse_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96115, 9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reprintdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_reuse</th>\n",
       "      <th>pair_index1</th>\n",
       "      <th>pair_index2</th>\n",
       "      <th>pair_date1</th>\n",
       "      <th>pair_date2</th>\n",
       "      <th>pair_text1</th>\n",
       "      <th>pair_text2</th>\n",
       "      <th>pair_state1</th>\n",
       "      <th>pair_state2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>72742</td>\n",
       "      <td>18910826</td>\n",
       "      <td>18910826</td>\n",
       "      <td>[tho, steamer, namchow, sail, port, chinese, c...</td>\n",
       "      <td>[bteamer, namchow, sail, port, chinese, coolie...</td>\n",
       "      <td>[New York]</td>\n",
       "      <td>[New York]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>31542</td>\n",
       "      <td>18841025</td>\n",
       "      <td>18841029</td>\n",
       "      <td>[chinese, general, send, soldier, disguise, co...</td>\n",
       "      <td>[chinese, general, send, soldier, disguise, co...</td>\n",
       "      <td>[Montana]</td>\n",
       "      <td>[South Dakota]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_reuse  pair_index1  pair_index2  pair_date1  pair_date2  \\\n",
       "0         3            1        72742    18910826    18910826   \n",
       "1         3            6        31542    18841025    18841029   \n",
       "\n",
       "                                          pair_text1  \\\n",
       "0  [tho, steamer, namchow, sail, port, chinese, c...   \n",
       "1  [chinese, general, send, soldier, disguise, co...   \n",
       "\n",
       "                                          pair_text2 pair_state1  \\\n",
       "0  [bteamer, namchow, sail, port, chinese, coolie...  [New York]   \n",
       "1  [chinese, general, send, soldier, disguise, co...   [Montana]   \n",
       "\n",
       "      pair_state2  \n",
       "0      [New York]  \n",
       "1  [South Dakota]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reprintdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprintdf.to_csv(path+'reprint.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coolie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
